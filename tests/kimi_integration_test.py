#!/usr/bin/env python3
"""
Kimi K2 LLMé›†æˆæµ‹è¯•
====================

æµ‹è¯•Kimi (æœˆä¹‹æš—é¢) LLMæä¾›å•†çš„é›†æˆæ•ˆæœ
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import settings
from orchestration.llm_gateway import create_llm_gateway, LLMMessage
from agent.agents.chatbot.agent import ChatBotAgent


async def test_kimi_llm_integration():
    """æµ‹è¯•Kimi LLMé›†æˆ"""
    print("ğŸš€ Kimi K2 LLMé›†æˆæµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºLLMç½‘å…³
    llm_gateway = create_llm_gateway(settings)
    await llm_gateway.initialize()
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡å’Œæä¾›å•†
    mock_llm = os.getenv("MOCK_LLM", "false").lower() == "true"
    print(f"ğŸ”§ MOCK_LLMæ¨¡å¼: {mock_llm}")
    print(f"ğŸ”§ å¯ç”¨æä¾›å•†: {list(llm_gateway.providers.keys())}")
    print(f"ğŸ”§ é»˜è®¤æä¾›å•†: {llm_gateway.config.get('default_provider', 'openai')}")
    
    if "kimi" in llm_gateway.providers:
        print("âœ… Kimiæä¾›å•†å·²æˆåŠŸæ³¨å†Œ")
    else:
        print("âŒ Kimiæä¾›å•†æœªæ‰¾åˆ°")
    
    print("\n" + "-" * 50)
    
    # æµ‹è¯•1: ç›´æ¥LLMç½‘å…³è°ƒç”¨
    print("ğŸ“¡ æµ‹è¯•1: ç›´æ¥è°ƒç”¨Kimi LLMç½‘å…³")
    test_messages_direct = [
        LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹"),
        LLMMessage(role="user", content="è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼Œè¯´æ˜ä½ çš„èƒ½åŠ›")
    ]
    
    try:
        if mock_llm:
            response = await llm_gateway.generate(test_messages_direct, provider="kimi")
        else:
            # çœŸå®æ¨¡å¼ä¸‹ï¼Œå¦‚æœæœ‰kimié…ç½®
            if llm_gateway.providers.get("kimi"):
                response = await llm_gateway.generate(test_messages_direct, provider="kimi")
            else:
                print("âš ï¸  çœŸå®æ¨¡å¼ä¸‹æœªé…ç½®Kimi APIå¯†é’¥ï¼Œè·³è¿‡ç›´æ¥è°ƒç”¨æµ‹è¯•")
                response = None
        
        if response:
            print(f"ğŸ¤– Kimiå›å¤: {response.content}")
            print(f"ğŸ“Š æ¨¡å‹: {response.model}")
            print(f"ğŸ·ï¸  æä¾›å•†: {response.provider}")
            if response.usage:
                print(f"ğŸ“ˆ Tokenä½¿ç”¨: {response.usage}")
    except Exception as e:
        print(f"âŒ ç›´æ¥è°ƒç”¨å¤±è´¥: {e}")
    
    print("\n" + "-" * 50)
    
    # æµ‹è¯•2: é€šè¿‡ChatBoté›†æˆæµ‹è¯•
    print("ğŸ¤– æµ‹è¯•2: ChatBoté›†æˆKimi LLM")
    
    chatbot = ChatBotAgent(
        agent_id="kimi_test_chatbot",
        name="Kimiæµ‹è¯•ChatBot",
        description="é›†æˆKimi LLMçš„èŠå¤©æœºå™¨äºº"
    )
    
    # æ³¨å…¥ä¾èµ–
    chatbot.inject_dependencies(llm_gateway, None, None)
    await chatbot.initialize()
    
    test_conversations = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹Kimi K2çš„ç‰¹ç‚¹",
        "ä½ èƒ½å¤„ç†å“ªäº›ç±»å‹çš„é—®é¢˜ï¼Ÿ",
        "è¯·å¸®æˆ‘å†™ä¸€ä¸ªç®€å•çš„Pythonå‡½æ•°",
        "è°¢è°¢ä½ çš„å¸®åŠ©"
    ]
    
    session_id = "kimi_test_session"
    
    for i, user_message in enumerate(test_conversations, 1):
        print(f"\nã€å¯¹è¯ {i}ã€‘")
        print(f"ğŸ‘¤ ç”¨æˆ·: {user_message}")
        
        try:
            # å¤„ç†æ¶ˆæ¯
            result = await chatbot.process({
                "message": user_message,
                "session_id": session_id
            })
            
            if result["status"] == "success":
                print(f"ğŸ¤– ChatBot: {result['response']}")
                print(f"ğŸ“Š å¯¹è¯å†å²é•¿åº¦: {len(chatbot.conversation_manager.get_conversation_history(session_id))}")
            else:
                print(f"âŒ ChatBoté”™è¯¯: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                
        except Exception as e:
            print(f"âŒ å¯¹è¯å¤„ç†å¤±è´¥: {e}")
        
        # æ·»åŠ çŸ­æš‚å»¶è¿Ÿ
        await asyncio.sleep(0.5)
    
    print("\n" + "-" * 50)
    
    # æµ‹è¯•3: æ€§èƒ½å’ŒåŠŸèƒ½ç‰¹æ€§æµ‹è¯•
    print("âš¡ æµ‹è¯•3: KimiåŠŸèƒ½ç‰¹æ€§æµ‹è¯•")
    
    feature_tests = [
        {
            "name": "é•¿æ–‡æœ¬ç†è§£",
            "message": "è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹çš„è¦ç‚¹ï¼šäººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨è¿‘å¹´æ¥å–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œä¸ºå„è¡Œå„ä¸šå¸¦æ¥äº†æ–°çš„æœºé‡å’ŒæŒ‘æˆ˜ã€‚è¿™äº›æ¨¡å‹ä¸ä»…èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€ï¼Œè¿˜èƒ½å¤Ÿè¿›è¡Œé€»è¾‘æ¨ç†ã€åˆ›æ„å†™ä½œã€ä»£ç ç¼–ç¨‹ç­‰å¤æ‚ä»»åŠ¡ã€‚"
        },
        {
            "name": "ä»£ç ç”Ÿæˆ",
            "message": "è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°"
        },
        {
            "name": "å¤šè½®å¯¹è¯è®°å¿†",
            "message": "æˆ‘ä¹‹å‰é—®è¿‡ä½ ä»€ä¹ˆé—®é¢˜ï¼Ÿè¯·å›é¡¾æˆ‘ä»¬çš„å¯¹è¯å†å²"
        }
    ]
    
    for test in feature_tests:
        print(f"\nğŸ§ª {test['name']}æµ‹è¯•:")
        print(f"ğŸ‘¤ ç”¨æˆ·: {test['message']}")
        
        try:
            result = await chatbot.process({
                "message": test['message'],
                "session_id": session_id
            })
            
            if result["status"] == "success":
                # æˆªæ–­é•¿å›å¤
                response = result['response']
                if len(response) > 200:
                    response = response[:200] + "..."
                print(f"ğŸ¤– ChatBot: {response}")
                print("âœ… æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {result.get('message')}")
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
    
    # æ¸…ç†èµ„æº
    await chatbot.shutdown()
    await llm_gateway.shutdown()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ Kimi K2 LLMé›†æˆæµ‹è¯•å®Œæˆï¼")
    
    # æ€»ç»“
    print("\nğŸ“‹ é›†æˆæ€»ç»“:")
    print("âœ… KimiProviderç±»å®ç°å®Œæˆ")
    print("âœ… é…ç½®ç®¡ç†æ”¯æŒå®Œæˆ") 
    print("âœ… LLMç½‘å…³æ³¨å†Œå®Œæˆ")
    print("âœ… ChatBoté›†æˆæµ‹è¯•å®Œæˆ")
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    if mock_llm:
        print("- å½“å‰ä¸ºæ¨¡æ‹Ÿæ¨¡å¼ï¼Œæ‰€æœ‰Kimiè°ƒç”¨ä½¿ç”¨MockLLMProvider")
        print("- è¦ä½¿ç”¨çœŸå®Kimi APIï¼Œè¯·è®¾ç½®MOCK_LLM=falseå¹¶é…ç½®æœ‰æ•ˆçš„KIMI_API_KEY")
    else:
        print("- å½“å‰ä¸ºçœŸå®æ¨¡å¼ï¼Œéœ€è¦æœ‰æ•ˆçš„Kimi APIå¯†é’¥")
        print("- è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®KIMI_API_KEY")
    print("- æ”¯æŒçš„Kimiæ¨¡å‹: moonshot-v1-8k, moonshot-v1-32k, moonshot-v1-128k")


async def test_provider_switching():
    """æµ‹è¯•æä¾›å•†åˆ‡æ¢åŠŸèƒ½"""
    print("\nğŸ”„ æµ‹è¯•æä¾›å•†åˆ‡æ¢åŠŸèƒ½")
    print("-" * 30)
    
    llm_gateway = create_llm_gateway(settings)
    await llm_gateway.initialize()
    
    test_message = [LLMMessage(role="user", content="ä½ æ˜¯å“ªä¸ªAIåŠ©æ‰‹ï¼Ÿ")]
    
    providers_to_test = ["openai", "claude", "kimi"]
    
    for provider in providers_to_test:
        if provider in llm_gateway.providers:
            print(f"\nğŸ”§ åˆ‡æ¢åˆ° {provider} æä¾›å•†:")
            try:
                response = await llm_gateway.generate(test_message, provider=provider)
                print(f"âœ… {provider}: {response.content[:100]}...")
            except Exception as e:
                print(f"âŒ {provider} è°ƒç”¨å¤±è´¥: {e}")
        else:
            print(f"âš ï¸  {provider} æä¾›å•†æœªæ³¨å†Œ")
    
    await llm_gateway.shutdown()


if __name__ == "__main__":
    asyncio.run(test_kimi_llm_integration())
    print("\n" + "=" * 50)
    asyncio.run(test_provider_switching())