#!/usr/bin/env python3
"""
Kimi K2 åŠŸèƒ½æ¼”ç¤º
================

å±•ç¤ºKimié›†æˆåˆ°Puqeeæ¡†æ¶çš„å„ç§åŠŸèƒ½
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„  
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import settings
from orchestration.llm_gateway import create_llm_gateway, LLMMessage


async def demonstrate_kimi_features():
    """æ¼”ç¤ºKimiçš„å„é¡¹åŠŸèƒ½"""
    print("ğŸš€ Kimi K2 LLM åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    print("ğŸŒ™ é›†æˆæœˆä¹‹æš—é¢ Kimi å¤§è¯­è¨€æ¨¡å‹")
    print("ğŸ“‹ æ”¯æŒçš„æ¨¡å‹:")
    print("   â€¢ moonshot-v1-8k (8Kä¸Šä¸‹æ–‡)")
    print("   â€¢ moonshot-v1-32k (32Kä¸Šä¸‹æ–‡)") 
    print("   â€¢ moonshot-v1-128k (128Kä¸Šä¸‹æ–‡)")
    print("=" * 60)
    
    # åˆ›å»ºLLMç½‘å…³
    llm_gateway = create_llm_gateway(settings)
    await llm_gateway.initialize()
    
    # æ£€æŸ¥Kimiæä¾›å•†çŠ¶æ€
    mock_mode = os.getenv("MOCK_LLM", "false").lower() == "true"
    
    if mock_mode:
        print("ğŸ”§ å½“å‰è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼")
        print("ğŸ’¡ è¦ä½¿ç”¨çœŸå®Kimi APIï¼Œè¯·:")
        print("   1. è®¾ç½® MOCK_LLM=false")
        print("   2. é…ç½®æœ‰æ•ˆçš„ KIMI_API_KEY")
        print("   3. é‡æ–°è¿è¡Œæ¼”ç¤º")
    else:
        if "kimi" in llm_gateway.providers:
            print("âœ… Kimiæä¾›å•†å·²æ¿€æ´»ï¼Œä½¿ç”¨çœŸå®API")
        else:
            print("âš ï¸  Kimiæä¾›å•†æœªé…ç½®ï¼Œè¯·æ£€æŸ¥KIMI_API_KEY")
    
    print("\n" + "-" * 60)
    
    # åŠŸèƒ½æ¼”ç¤ºåœºæ™¯
    demo_scenarios = [
        {
            "title": "ğŸ¤– åŸºç¡€å¯¹è¯èƒ½åŠ›", 
            "messages": [
                LLMMessage(role="system", content="ä½ æ˜¯Kimiï¼Œæœˆä¹‹æš—é¢å¼€å‘çš„AIåŠ©æ‰‹"),
                LLMMessage(role="user", content="è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼ŒåŒ…æ‹¬ä½ çš„èƒ½åŠ›å’Œç‰¹è‰²")
            ]
        },
        {
            "title": "ğŸ’» ä»£ç ç”Ÿæˆèƒ½åŠ›",
            "messages": [
                LLMMessage(role="user", content="è¯·ç”¨Pythonå†™ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾ç®—æ³•ï¼Œè¦æ±‚åŒ…å«è¯¦ç»†æ³¨é‡Š")
            ]
        },
        {
            "title": "ğŸ“š æ–‡æœ¬åˆ†æèƒ½åŠ›", 
            "messages": [
                LLMMessage(role="user", content="""
è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦è§‚ç‚¹ï¼š

äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•æ­£åœ¨é‡å¡‘å„ä¸ªè¡Œä¸šã€‚ä»åŒ»ç–—è¯Šæ–­åˆ°é‡‘èé£é™©æ§åˆ¶ï¼Œ
ä»æ™ºèƒ½åˆ¶é€ åˆ°æ•™è‚²ä¸ªæ€§åŒ–ï¼ŒAIçš„åº”ç”¨åœºæ™¯è¶Šæ¥è¶Šå¹¿æ³›ã€‚ç„¶è€Œï¼ŒæŠ€æœ¯è¿›æ­¥
ä¹Ÿå¸¦æ¥äº†æ•°æ®éšç§ã€ç®—æ³•åè§ã€å°±ä¸šç»“æ„å˜åŒ–ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬éœ€è¦åœ¨æ¨åŠ¨
æŠ€æœ¯åˆ›æ–°çš„åŒæ—¶ï¼Œå»ºç«‹ç›¸åº”çš„ä¼¦ç†æ¡†æ¶å’Œç›‘ç®¡æœºåˆ¶ã€‚
                """)
            ]
        },
        {
            "title": "ğŸ” é€»è¾‘æ¨ç†èƒ½åŠ›",
            "messages": [
                LLMMessage(role="user", content="""
é€»è¾‘é¢˜ï¼šæœ‰Aã€Bã€Cä¸‰ä¸ªäººï¼Œå…¶ä¸­ï¼š
- Aè¯´ï¼šæˆ‘ä¸æ˜¯ç½ªçŠ¯
- Bè¯´ï¼šCæ˜¯ç½ªçŠ¯  
- Cè¯´ï¼šBåœ¨æ’’è°

å·²çŸ¥åªæœ‰ä¸€ä¸ªäººæ˜¯ç½ªçŠ¯ï¼Œä¸”ç½ªçŠ¯ä¼šè¯´è°ï¼Œæ— è¾œçš„äººè¯´çœŸè¯ã€‚
è¯·åˆ†æè°æ˜¯ç½ªçŠ¯ï¼Ÿ
                """)
            ]
        }
    ]
    
    # æ‰§è¡Œæ¼”ç¤º
    for i, scenario in enumerate(demo_scenarios, 1):
        print(f"\nğŸ“‹ åœºæ™¯ {i}: {scenario['title']}")
        print("-" * 40)
        
        # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
        user_msg = [msg for msg in scenario['messages'] if msg.role == 'user'][-1]
        print(f"ğŸ‘¤ ç”¨æˆ·è¾“å…¥:")
        print(f"   {user_msg.content.strip()}")
        
        try:
            # è°ƒç”¨Kimi LLM
            response = await llm_gateway.generate(
                scenario['messages'], 
                provider="kimi" if "kimi" in llm_gateway.providers else None
            )
            
            print(f"\nğŸ¤– Kimiå›å¤:")
            # æ ¼å¼åŒ–é•¿æ–‡æœ¬è¾“å‡º
            content = response.content
            if len(content) > 500:
                # æˆªæ–­é•¿å›å¤å¹¶æ˜¾ç¤ºå‰éƒ¨åˆ†
                lines = content.split('\n')
                displayed_lines = []
                char_count = 0
                
                for line in lines:
                    if char_count + len(line) > 400:
                        if char_count > 200:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹æ˜¾ç¤º
                            displayed_lines.append("   ...")
                            displayed_lines.append(f"   [å›å¤è¿‡é•¿ï¼Œå·²æˆªæ–­ã€‚å®Œæ•´é•¿åº¦: {len(content)} å­—ç¬¦]")
                            break
                    displayed_lines.append(f"   {line}")
                    char_count += len(line)
                
                print('\n'.join(displayed_lines))
            else:
                # çŸ­å›å¤ç›´æ¥æ˜¾ç¤º
                for line in content.split('\n'):
                    print(f"   {line}")
            
            print(f"\nğŸ“Š å“åº”ä¿¡æ¯:")
            print(f"   æ¨¡å‹: {response.model}")
            print(f"   æä¾›å•†: {response.provider}")
            if response.usage:
                print(f"   Tokenä½¿ç”¨: {response.usage}")
            
        except Exception as e:
            print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
        
        # æ·»åŠ åˆ†éš”ç¬¦
        if i < len(demo_scenarios):
            print("\n" + "Â·" * 40)
            await asyncio.sleep(0.5)  # é¿å…è¿‡å¿«è°ƒç”¨
    
    print("\n" + "=" * 60)
    
    # æ€§èƒ½ç‰¹æ€§è¯´æ˜
    print("ğŸŒŸ Kimi ç‰¹è‰²åŠŸèƒ½:")
    print("âœ¨ è¶…é•¿ä¸Šä¸‹æ–‡: æ”¯æŒæœ€é«˜128K tokençš„ä¸Šä¸‹æ–‡é•¿åº¦")
    print("ğŸ§  å¼ºå¤§æ¨ç†: æ“…é•¿é€»è¾‘åˆ†æå’Œå¤æ‚é—®é¢˜è§£å†³")
    print("ğŸ’» ä»£ç èƒ½åŠ›: æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€çš„ç”Ÿæˆå’Œè§£æ")
    print("ğŸ“– æ–‡æ¡£ç†è§£: èƒ½å¤Ÿå¤„ç†å’Œåˆ†æé•¿ç¯‡æ–‡æ¡£")
    print("ğŸŒ ä¸­æ–‡ä¼˜åŒ–: é’ˆå¯¹ä¸­æ–‡åœºæ™¯è¿›è¡Œäº†ç‰¹åˆ«ä¼˜åŒ–")
    
    print("\nğŸ”§ é›†æˆé…ç½®:")
    print("ğŸ“ é…ç½®æ–‡ä»¶: .env æˆ– .env.example")
    print("ğŸ”‘ APIå¯†é’¥: KIMI_API_KEY (éœ€è¦ä»æœˆä¹‹æš—é¢å®˜ç½‘è·å–)")
    print("ğŸŒ APIåœ°å€: https://api.moonshot.cn")
    print("âš™ï¸  æ¨¡å‹é€‰æ‹©: moonshot-v1-8k / 32k / 128k")
    
    # æ¸…ç†èµ„æº
    await llm_gateway.shutdown()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ Kimi K2 åŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(demonstrate_kimi_features())