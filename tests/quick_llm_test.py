#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•æ¨¡æ‹ŸLLMé›†æˆ
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import settings
from orchestration.llm_gateway import create_llm_gateway, LLMMessage


async def test_mock_llm():
    print("ğŸ§ª æµ‹è¯•æ¨¡æ‹ŸLLMé›†æˆ")
    print("=" * 40)
    
    # åˆ›å»ºLLMç½‘å…³
    llm_gateway = create_llm_gateway(settings)
    await llm_gateway.initialize()
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    mock_llm = os.getenv("MOCK_LLM", "false").lower() == "true"
    print(f"ğŸ”§ MOCK_LLM: {mock_llm}")
    print(f"ğŸ”§ æä¾›å•†æ•°é‡: {len(llm_gateway.providers)}")
    print(f"ğŸ”§ å¯ç”¨æä¾›å•†: {list(llm_gateway.providers.keys())}")
    
    if not llm_gateway.providers:
        print("âŒ é”™è¯¯: æ²¡æœ‰æ³¨å†Œä»»ä½•æä¾›å•†")
        return
    
    # æµ‹è¯•å¯¹è¯
    test_messages = [
        LLMMessage(role="user", content="ä½ å¥½"),
        LLMMessage(role="user", content="ä½ æ˜¯è°ï¼Ÿ"),
        LLMMessage(role="user", content="å†è§")
    ]
    
    for i, msg in enumerate(test_messages, 1):
        print(f"\nã€æµ‹è¯• {i}ã€‘")
        print(f"ğŸ‘¤ ç”¨æˆ·: {msg.content}")
        
        try:
            response = await llm_gateway.generate([msg])
            print(f"ğŸ¤– AI: {response.content}")
            print(f"ğŸ“Š æ¨¡å‹: {response.model} (æä¾›å•†: {response.provider})")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
    
    await llm_gateway.shutdown()
    print("\nâœ… æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    asyncio.run(test_mock_llm())