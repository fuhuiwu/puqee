#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMç½‘å…³æ¨¡å—
===========

è´Ÿè´£ç»Ÿä¸€ç®¡ç†å„ç§å¤§è¯­è¨€æ¨¡å‹çš„æ¥å…¥å’Œè°ƒç”¨
"""

import asyncio
import logging
import json
import aiohttp
import time
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from utils.logger import get_logger


@dataclass
class LLMMessage:
    """LLMæ¶ˆæ¯æ•°æ®ç»“æ„"""
    role: str  # system, user, assistant
    content: str
    
    def to_dict(self) -> Dict[str, Any]:
        return {"role": self.role, "content": self.content}


@dataclass
class LLMResponse:
    """LLMå“åº”æ•°æ®ç»“æ„"""
    content: str
    model: str
    provider: str
    usage: Dict[str, Any] = None
    metadata: Dict[str, Any] = None
    

class LLMProvider:
    """LLMæä¾›å•†åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = get_logger(f"puqee.llm.{self.get_provider_name()}")
    
    def get_provider_name(self) -> str:
        """è·å–æä¾›å•†åç§°"""
        raise NotImplementedError
    
    async def generate(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """ç”Ÿæˆå›å¤"""
        raise NotImplementedError
    
    async def validate_config(self) -> bool:
        """éªŒè¯é…ç½®"""
        raise NotImplementedError


class OpenAIProvider(LLMProvider):
    """OpenAIæä¾›å•†"""
    
    def get_provider_name(self) -> str:
        return "openai"
    
    async def validate_config(self) -> bool:
        """éªŒè¯OpenAIé…ç½®"""
        required_keys = ["api_key", "api_base", "model"]
        return all(key in self.config for key in required_keys)
    
    async def generate(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """è°ƒç”¨OpenAI APIç”Ÿæˆå›å¤"""
        if not await self.validate_config():
            raise ValueError("OpenAIé…ç½®ä¸å®Œæ•´")
        
        headers = {
            "Authorization": f"Bearer {self.config['api_key']}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.config.get("model", "gpt-3.5-turbo"),
            "messages": [msg.to_dict() for msg in messages],
            "max_tokens": self.config.get("max_tokens", 2000),
            "temperature": self.config.get("temperature", 0.7),
            **kwargs
        }
        
        url = f"{self.config['api_base']}/chat/completions"
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    url, 
                    headers=headers, 
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=self.config.get("timeout", 30))
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        return LLMResponse(
                            content=data["choices"][0]["message"]["content"],
                            model=data["model"],
                            provider="openai",
                            usage=data.get("usage", {}),
                            metadata={"response_id": data.get("id")}
                        )
                    else:
                        error_text = await response.text()
                        raise Exception(f"OpenAI APIé”™è¯¯ {response.status}: {error_text}")
                        
            except asyncio.TimeoutError:
                raise Exception("OpenAI APIè¯·æ±‚è¶…æ—¶")
            except Exception as e:
                self.logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
                raise


class ClaudeProvider(LLMProvider):
    """Claudeæä¾›å•†"""
    
    def get_provider_name(self) -> str:
        return "claude"
    
    async def validate_config(self) -> bool:
        """éªŒè¯Claudeé…ç½®"""
        required_keys = ["api_key", "api_base", "model"]
        return all(key in self.config for key in required_keys)
    
    async def generate(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """è°ƒç”¨Claude APIç”Ÿæˆå›å¤"""
        if not await self.validate_config():
            raise ValueError("Claudeé…ç½®ä¸å®Œæ•´")
        
        headers = {
            "x-api-key": self.config['api_key'],
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        # Claude APIæ ¼å¼è½¬æ¢
        system_message = None
        conversation_messages = []
        
        for msg in messages:
            if msg.role == "system":
                system_message = msg.content
            else:
                conversation_messages.append(msg.to_dict())
        
        payload = {
            "model": self.config.get("model", "claude-3-haiku-20240307"),
            "max_tokens": self.config.get("max_tokens", 2000),
            "messages": conversation_messages,
            **kwargs
        }
        
        if system_message:
            payload["system"] = system_message
        
        url = f"{self.config['api_base']}/v1/messages"
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    url, 
                    headers=headers, 
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=self.config.get("timeout", 30))
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        content = ""
                        if data.get("content") and len(data["content"]) > 0:
                            content = data["content"][0].get("text", "")
                        
                        return LLMResponse(
                            content=content,
                            model=data.get("model", "claude"),
                            provider="claude",
                            usage=data.get("usage", {}),
                            metadata={"response_id": data.get("id")}
                        )
                    else:
                        error_text = await response.text()
                        raise Exception(f"Claude APIé”™è¯¯ {response.status}: {error_text}")
                        
            except asyncio.TimeoutError:
                raise Exception("Claude APIè¯·æ±‚è¶…æ—¶")
            except Exception as e:
                self.logger.error(f"Claude APIè°ƒç”¨å¤±è´¥: {e}")
                raise


class KimiProvider(LLMProvider):
    """Kimi (æœˆä¹‹æš—é¢) æä¾›å•†"""
    
    def get_provider_name(self) -> str:
        return "kimi"
    
    async def validate_config(self) -> bool:
        """éªŒè¯Kimié…ç½®"""
        required_keys = ["api_key", "api_base", "model"]
        return all(key in self.config for key in required_keys)
    
    async def generate(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """è°ƒç”¨Kimi APIç”Ÿæˆå›å¤"""
        if not await self.validate_config():
            raise ValueError("Kimié…ç½®ä¸å®Œæ•´")
        
        headers = {
            "Authorization": f"Bearer {self.config['api_key']}",
            "Content-Type": "application/json"
        }
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼ˆKimiä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼ï¼‰
        api_messages = [msg.to_dict() for msg in messages]
        
        payload = {
            "model": self.config.get("model", "moonshot-v1-8k"),
            "messages": api_messages,
            "max_tokens": self.config.get("max_tokens", 2000),
            "temperature": self.config.get("temperature", 0.7),
            "stream": False,
            **kwargs
        }
        
        url = f"{self.config['api_base']}/v1/chat/completions"
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    url, 
                    headers=headers, 
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=self.config.get("timeout", 30))
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        return LLMResponse(
                            content=data["choices"][0]["message"]["content"],
                            model=data["model"],
                            provider="kimi",
                            usage=data.get("usage", {}),
                            metadata={"response_id": data.get("id")}
                        )
                    else:
                        error_text = await response.text()
                        raise Exception(f"Kimi APIé”™è¯¯ {response.status}: {error_text}")
                        
            except asyncio.TimeoutError:
                raise Exception("Kimi APIè¯·æ±‚è¶…æ—¶")
            except Exception as e:
                self.logger.error(f"Kimi APIè°ƒç”¨å¤±è´¥: {e}")
                raise


class MockLLMProvider(LLMProvider):
    """æ¨¡æ‹ŸLLMæä¾›å•†ï¼Œç”¨äºå¼€å‘å’Œæµ‹è¯•"""
    
    def get_provider_name(self) -> str:
        return "mock"
    
    async def validate_config(self) -> bool:
        """éªŒè¯æ¨¡æ‹Ÿé…ç½®"""
        return True  # æ¨¡æ‹Ÿæä¾›å•†ä¸éœ€è¦éªŒè¯
    
    async def generate(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """ç”Ÿæˆæ¨¡æ‹Ÿå›å¤"""
        # æ¨¡æ‹ŸAPIè°ƒç”¨å»¶è¿Ÿ
        await asyncio.sleep(0.1)
        
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_messages = [msg for msg in messages if msg.role == "user"]
        if not user_messages:
            return LLMResponse(
                content="æˆ‘æ²¡æœ‰æ”¶åˆ°æ‚¨çš„æ¶ˆæ¯ï¼Œè¯·é‡æ–°å‘é€ã€‚",
                model="mock-gpt-3.5",
                provider="mock"
            )
        
        last_user_message = user_messages[-1].content.lower()
        
        # æ ¹æ®ç”¨æˆ·æ¶ˆæ¯ç”Ÿæˆç›¸åº”çš„æ¨¡æ‹Ÿå›å¤
        if any(greeting in last_user_message for greeting in ["ä½ å¥½", "hello", "hi", "å—¨"]):
            response_content = "ä½ å¥½ï¼æˆ‘æ˜¯Puqeeçš„æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å›ç­”æ‚¨çš„é—®é¢˜ï¼Œè¿›è¡Œå¯¹è¯äº¤æµã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
        elif any(question in last_user_message for question in ["æ˜¯è°", "ä½ æ˜¯", "ä»‹ç»", "è‡ªå·±"]):
            response_content = "æˆ‘æ˜¯Puqeeæ¡†æ¶çš„æ™ºèƒ½ChatBotï¼ŒåŸºäºå…ˆè¿›çš„AIæŠ€æœ¯æ„å»ºã€‚æˆ‘å¯ä»¥è¿›è¡Œè‡ªç„¶å¯¹è¯ã€å›ç­”é—®é¢˜ã€æä¾›å¸®åŠ©å’Œå»ºè®®ã€‚"
        elif any(capability in last_user_message for capability in ["èƒ½åš", "åŠŸèƒ½", "èƒ½åŠ›", "å¸®åŠ©"]):
            response_content = "æˆ‘ç›®å‰å¯ä»¥æä¾›ä»¥ä¸‹æœåŠ¡ï¼š\nâ€¢ ğŸ’¬ è‡ªç„¶å¯¹è¯äº¤æµ\nâ€¢ ğŸ§  è®°å¿†å¯¹è¯ä¸Šä¸‹æ–‡\nâ€¢ â“ å›ç­”å„ç§é—®é¢˜\nâ€¢ ğŸ’¡ æä¾›å»ºè®®å’Œå¸®åŠ©\nâ€¢ ğŸ”§ æŠ€æœ¯ç›¸å…³å’¨è¯¢"
        elif any(tech in last_user_message for tech in ["python", "ç¼–ç¨‹", "ä»£ç ", "å¼€å‘", "æŠ€æœ¯"]):
            response_content = "å…³äºæŠ€æœ¯é—®é¢˜ï¼Œæˆ‘å¾ˆä¹æ„ä¸ºæ‚¨è§£ç­”ï¼Pythonæ˜¯ä¸€é—¨åŠŸèƒ½å¼ºå¤§çš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºWebå¼€å‘ã€æ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚æ‚¨æƒ³äº†è§£å“ªä¸ªæ–¹é¢å‘¢ï¼Ÿ"
        elif any(thanks in last_user_message for thanks in ["è°¢è°¢", "æ„Ÿè°¢", "thank"]):
            response_content = "ä¸å®¢æ°”ï¼å¾ˆé«˜å…´èƒ½å¸®åŠ©æ‚¨ã€‚å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚"
        elif any(goodbye in last_user_message for goodbye in ["å†è§", "æ‹œæ‹œ", "bye", "goodbye"]):
            response_content = "å†è§ï¼å¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼ŒæœŸå¾…ä¸‹æ¬¡äº¤æµï¼"
        else:
            response_content = f"æˆ‘ç†è§£æ‚¨æåˆ°äº†ï¼š{user_messages[-1].content}ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„è¯é¢˜ï¼ä½œä¸ºAIåŠ©æ‰‹ï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©å’Œä¿¡æ¯ã€‚æ‚¨å¸Œæœ›æˆ‘å…·ä½“å›ç­”ä»€ä¹ˆé—®é¢˜å‘¢ï¼Ÿ"
        
        return LLMResponse(
            content=response_content,
            model="mock-gpt-3.5-turbo", 
            provider="mock",
            usage={"prompt_tokens": 50, "completion_tokens": 100, "total_tokens": 150},
            metadata={"mock": True, "response_time": 0.1}
        )


class LLMGateway:
    """
    LLMç½‘å…³ç±»
    
    æä¾›ç»Ÿä¸€çš„LLMè®¿é—®æ¥å£ï¼Œæ”¯æŒå¤šç§LLMæä¾›å•†
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–LLMç½‘å…³"""
        self.logger = get_logger("puqee.llm_gateway")
        self.providers: Dict[str, LLMProvider] = {}
        self.config = config or {}
        self.default_provider = None
        
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–LLMç½‘å…³"""
        self.logger.info("æ­£åœ¨åˆå§‹åŒ–LLMç½‘å…³...")
        
        # æ³¨å†Œå¯ç”¨çš„æä¾›å•†
        self._register_providers()
        
        # åˆå§‹åŒ–é»˜è®¤æä¾›å•†
        default_provider_name = self.config.get("default_provider", "openai")
        if default_provider_name in self.providers:
            self.default_provider = self.providers[default_provider_name]
            self.logger.info(f"é»˜è®¤LLMæä¾›å•†: {default_provider_name}")
        else:
            self.logger.warning(f"é»˜è®¤LLMæä¾›å•† '{default_provider_name}' æœªæ‰¾åˆ°")
        
        self.logger.info("LLMç½‘å…³åˆå§‹åŒ–å®Œæˆ")
        
    def _register_providers(self):
        """æ³¨å†ŒLLMæä¾›å•†"""
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨¡æ‹Ÿæ¨¡å¼
        mock_llm = self.config.get("mock_llm", False)
        
        if mock_llm:
            # æ¨¡æ‹Ÿæ¨¡å¼ï¼šåªæ³¨å†Œæ¨¡æ‹Ÿæä¾›å•†
            mock_provider = MockLLMProvider({})
            self.providers["openai"] = mock_provider  # æ¨¡æ‹Ÿopenai
            self.providers["claude"] = mock_provider   # æ¨¡æ‹Ÿclaude  
            self.providers["kimi"] = mock_provider     # æ¨¡æ‹Ÿkimi
            self.providers["mock"] = mock_provider     # æ˜¾å¼æ¨¡æ‹Ÿæä¾›å•†
            self.logger.info("æ¨¡æ‹ŸLLMæä¾›å•†å·²æ³¨å†Œ")
        else:
            # çœŸå®æ¨¡å¼ï¼šæ³¨å†ŒçœŸå®æä¾›å•†
            # æ³¨å†ŒOpenAI
            if self.config.get("openai"):
                try:
                    openai_provider = OpenAIProvider(self.config["openai"])
                    self.providers["openai"] = openai_provider
                    self.logger.info("OpenAIæä¾›å•†å·²æ³¨å†Œ")
                except Exception as e:
                    self.logger.error(f"æ³¨å†ŒOpenAIæä¾›å•†å¤±è´¥: {e}")
            
            # æ³¨å†ŒClaude
            if self.config.get("claude"):
                try:
                    claude_provider = ClaudeProvider(self.config["claude"])
                    self.providers["claude"] = claude_provider
                    self.logger.info("Claudeæä¾›å•†å·²æ³¨å†Œ")
                except Exception as e:
                    self.logger.error(f"æ³¨å†ŒClaudeæä¾›å•†å¤±è´¥: {e}")
            
            # æ³¨å†ŒKimi
            if self.config.get("kimi"):
                try:
                    kimi_provider = KimiProvider(self.config["kimi"])
                    self.providers["kimi"] = kimi_provider
                    self.logger.info("Kimiæä¾›å•†å·²æ³¨å†Œ")
                except Exception as e:
                    self.logger.error(f"æ³¨å†ŒKimiæä¾›å•†å¤±è´¥: {e}")
    
    async def generate(
        self, 
        messages: List[LLMMessage], 
        provider: Optional[str] = None,
        retry_count: Optional[int] = None,
        **kwargs
    ) -> LLMResponse:
        """
        ç”ŸæˆLLMå›å¤
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            provider: æŒ‡å®šçš„æä¾›å•†åç§°ï¼Œé»˜è®¤ä½¿ç”¨é»˜è®¤æä¾›å•†
            retry_count: é‡è¯•æ¬¡æ•°
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            LLMResponse: LLMå“åº”
        """
        # é€‰æ‹©æä¾›å•†
        if provider and provider in self.providers:
            llm_provider = self.providers[provider]
        elif self.default_provider:
            llm_provider = self.default_provider
        else:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„LLMæä¾›å•†")
        
        # è®¾ç½®é‡è¯•å‚æ•°
        if retry_count is None:
            retry_count = self.config.get("retry_count", 3)
        
        retry_delay = self.config.get("retry_delay", 1.0)
        
        # å°è¯•ç”Ÿæˆå›å¤
        last_error = None
        for attempt in range(retry_count):
            try:
                if attempt > 0:
                    self.logger.info(f"é‡è¯•LLMè°ƒç”¨ï¼Œç¬¬ {attempt + 1}/{retry_count} æ¬¡")
                    await asyncio.sleep(retry_delay * attempt)
                
                response = await llm_provider.generate(messages, **kwargs)
                self.logger.debug(f"LLMç”ŸæˆæˆåŠŸ: {response.provider}/{response.model}")
                return response
                
            except Exception as e:
                last_error = e
                self.logger.warning(f"LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {e}")
                
                if attempt == retry_count - 1:
                    break
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        self.logger.error(f"LLMè°ƒç”¨å½»åº•å¤±è´¥: {last_error}")
        raise Exception(f"LLMè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•{retry_count}æ¬¡: {last_error}")
    
    async def generate_simple(
        self, 
        system_prompt: str,
        user_message: str,
        provider: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        ç®€åŒ–çš„ç”Ÿæˆæ¥å£
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_message: ç”¨æˆ·æ¶ˆæ¯
            provider: æä¾›å•†åç§°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            str: ç”Ÿæˆçš„å›å¤å†…å®¹
        """
        messages = []
        
        if system_prompt:
            messages.append(LLMMessage(role="system", content=system_prompt))
        
        messages.append(LLMMessage(role="user", content=user_message))
        
        response = await self.generate(messages, provider, **kwargs)
        return response.content
    
    def get_available_providers(self) -> List[str]:
        """è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨"""
        return list(self.providers.keys())
    
    def get_provider_info(self, provider_name: str) -> Dict[str, Any]:
        """è·å–æä¾›å•†ä¿¡æ¯"""
        if provider_name not in self.providers:
            return {}
        
        provider = self.providers[provider_name]
        return {
            "name": provider.get_provider_name(),
            "config": {k: "***" if "key" in k.lower() else v 
                      for k, v in provider.config.items()}
        }
        
    async def shutdown(self):
        """å…³é—­LLMç½‘å…³"""
        self.logger.info("æ­£åœ¨å…³é—­LLMç½‘å…³...")
        self.providers.clear()
        self.default_provider = None
        self.logger.info("LLMç½‘å…³å·²å…³é—­")


def create_llm_gateway(settings) -> LLMGateway:
    """
    ä»é…ç½®åˆ›å»ºLLMç½‘å…³
    
    Args:
        settings: é…ç½®å¯¹è±¡
        
    Returns:
        LLMGateway: é…ç½®å¥½çš„LLMç½‘å…³å®ä¾‹
    """
    import os
    
    config = {
        "default_provider": settings.DEFAULT_LLM_PROVIDER,
        "retry_count": settings.LLM_RETRY_COUNT,
        "retry_delay": settings.LLM_RETRY_DELAY,
    }
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨¡æ‹Ÿæ¨¡å¼
    mock_llm = os.getenv("MOCK_LLM", "false").lower() == "true"
    config["mock_llm"] = mock_llm
    
    if not mock_llm:
        # çœŸå®æ¨¡å¼ï¼šé…ç½®çœŸå®æä¾›å•†
        # OpenAIé…ç½®
        if settings.OPENAI_API_KEY:
            config["openai"] = {
                "api_key": settings.OPENAI_API_KEY,
                "api_base": settings.OPENAI_API_BASE,
                "model": settings.OPENAI_MODEL,
                "max_tokens": settings.OPENAI_MAX_TOKENS,
                "temperature": settings.OPENAI_TEMPERATURE,
                "timeout": settings.LLM_TIMEOUT,
            }
        
        # Claudeé…ç½®
        if settings.CLAUDE_API_KEY:
            config["claude"] = {
                "api_key": settings.CLAUDE_API_KEY,
                "api_base": settings.CLAUDE_API_BASE,
                "model": settings.CLAUDE_MODEL,
                "max_tokens": settings.CLAUDE_MAX_TOKENS,
                "timeout": settings.LLM_TIMEOUT,
            }
        
        # Kimié…ç½®
        if settings.KIMI_API_KEY:
            config["kimi"] = {
                "api_key": settings.KIMI_API_KEY,
                "api_base": settings.KIMI_API_BASE,
                "model": settings.KIMI_MODEL,
                "max_tokens": settings.KIMI_MAX_TOKENS,
                "temperature": settings.KIMI_TEMPERATURE,
                "timeout": settings.LLM_TIMEOUT,
            }
    
    return LLMGateway(config)